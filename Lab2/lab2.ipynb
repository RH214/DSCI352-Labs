{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 2: classification methods\n",
    "\n",
    "This lab is due by midnight Saturday Feb 19th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# You will need to change this for your environment\n",
    "DATA_ROOT = '../ALL CSV FILES - 2nd Edition/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-01-01</th>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-01</th>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-01</th>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-01</th>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-01</th>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today Direction\n",
       "Year                                                                  \n",
       "2001-01-01  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959        Up\n",
       "2001-01-01  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032        Up\n",
       "2001-01-01  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623      Down\n",
       "2001-01-01 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614        Up\n",
       "2001-01-01  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213        Up"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the 'index_col' argument here, which makes slicing easier below.\n",
    "market = pd.read_csv(DATA_ROOT + 'Smarket.csv', index_col=0, parse_dates=True)\n",
    "market.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Generalized Linear Model Regression Results                           \n",
      "================================================================================================\n",
      "Dep. Variable:     ['Direction[Down]', 'Direction[Up]']   No. Observations:                 1250\n",
      "Model:                                              GLM   Df Residuals:                     1243\n",
      "Model Family:                                  Binomial   Df Model:                            6\n",
      "Link Function:                                    logit   Scale:                          1.0000\n",
      "Method:                                            IRLS   Log-Likelihood:                -863.79\n",
      "Date:                                  Sat, 12 Feb 2022   Deviance:                       1727.6\n",
      "Time:                                          20:20:51   Pearson chi2:                 1.25e+03\n",
      "No. Iterations:                                       4                                         \n",
      "Covariance Type:                              nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.1260      0.241      0.523      0.601      -0.346       0.598\n",
      "Lag1           0.0731      0.050      1.457      0.145      -0.025       0.171\n",
      "Lag2           0.0423      0.050      0.845      0.398      -0.056       0.140\n",
      "Lag3          -0.0111      0.050     -0.222      0.824      -0.109       0.087\n",
      "Lag4          -0.0094      0.050     -0.187      0.851      -0.107       0.089\n",
      "Lag5          -0.0103      0.050     -0.208      0.835      -0.107       0.087\n",
      "Volume        -0.1354      0.158     -0.855      0.392      -0.446       0.175\n",
      "==============================================================================\n",
      "predicted probabilities: [0.49291587 0.51853212 0.51886117 0.48477764 0.48921884 0.49304354\n",
      " 0.50734913 0.49077084 0.48238647 0.51116222]\n",
      "qualitative predictions: ['Up', 'Down', 'Down', 'Up', 'Up', 'Up', 'Down', 'Up', 'Up', 'Down']\n",
      "confusion matrix:\n",
      " [[145 141]\n",
      " [457 507]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.507     0.241     0.327       602\n",
      "          Up      0.526     0.782     0.629       648\n",
      "\n",
      "    accuracy                          0.522      1250\n",
      "   macro avg      0.516     0.512     0.478      1250\n",
      "weighted avg      0.517     0.522     0.483      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will re-use this formula with other learning methods below\n",
    "all_lags = 'Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume'\n",
    "\n",
    "marklr = smf.glm(formula=all_lags, data=market, family=sm.families.Binomial())\n",
    "mlr_res = marklr.fit()\n",
    "print(mlr_res.summary())\n",
    "\n",
    "# The predicted values are probabilities\n",
    "mlr_prob = mlr_res.predict()\n",
    "print('predicted probabilities:', mlr_prob[0:10])\n",
    "\n",
    "# Here we create a set of qualitative predictions by thresholding on the probabilities\n",
    "predictions_nominal = [\"Up\" if x < 0.5 else \"Down\" for x in mlr_prob]\n",
    "print('qualitative predictions:', predictions_nominal[0:10])\n",
    "\n",
    "# Note: the '.T' here to take the transpose so that the true classes are columns and the predicted classes are rows,\n",
    "# matching the class slides\n",
    "print('confusion matrix:\\n', confusion_matrix(market[\"Direction\"], predictions_nominal).T)\n",
    "\n",
    "print(classification_report(market[\"Direction\"], predictions_nominal, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets, training on everything up to and including 2004 data\n",
    "# and testing on 2005 and later data:\n",
    "x_train = market[:'2004'][:]\n",
    "y_train = market[:'2004']['Direction']\n",
    "\n",
    "x_test = market['2005':][:]\n",
    "y_test = market['2005':]['Direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Generalized Linear Model Regression Results                           \n",
      "================================================================================================\n",
      "Dep. Variable:     ['Direction[Down]', 'Direction[Up]']   No. Observations:                  998\n",
      "Model:                                              GLM   Df Residuals:                      991\n",
      "Model Family:                                  Binomial   Df Model:                            6\n",
      "Link Function:                                    logit   Scale:                          1.0000\n",
      "Method:                                            IRLS   Log-Likelihood:                -690.55\n",
      "Date:                                  Sat, 12 Feb 2022   Deviance:                       1381.1\n",
      "Time:                                          20:30:57   Pearson chi2:                     998.\n",
      "No. Iterations:                                       4                                         \n",
      "Covariance Type:                              nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.1912      0.334     -0.573      0.567      -0.845       0.463\n",
      "Lag1           0.0542      0.052      1.046      0.295      -0.047       0.156\n",
      "Lag2           0.0458      0.052      0.884      0.377      -0.056       0.147\n",
      "Lag3          -0.0072      0.052     -0.139      0.889      -0.108       0.094\n",
      "Lag4          -0.0064      0.052     -0.125      0.901      -0.108       0.095\n",
      "Lag5           0.0042      0.051      0.083      0.934      -0.096       0.104\n",
      "Volume         0.1163      0.240      0.485      0.628      -0.353       0.586\n",
      "==============================================================================\n",
      "confusion matrix:\n",
      " [[77 97]\n",
      " [34 44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.44      0.69      0.54       111\n",
      "          Up       0.56      0.31      0.40       141\n",
      "\n",
      "    accuracy                           0.48       252\n",
      "   macro avg       0.50      0.50      0.47       252\n",
      "weighted avg       0.51      0.48      0.46       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit a logistic regression to the training data and (below) evaluate it using the test data\n",
    "mlr_04 = smf.glm(formula=all_lags, data=x_train, family=sm.families.Binomial())\n",
    "res_04 = mlr_04.fit()\n",
    "print(res_04.summary())\n",
    "\n",
    "# Build predictions of the test data using a 0.5 threshold\n",
    "prob_04 = res_04.predict(x_test)\n",
    "pred_04 = ['Up' if x < 0.5 else 'Down' for x in prob_04]\n",
    "\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_04).T)\n",
    "print(classification_report(y_test, pred_04))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Your job: build and test a LR model with only the two predictors with the best p-values above\n",
    "\n",
    "Looking at the model summary above, that will be Lag1 and Lag2.\n",
    "\n",
    "Build the new model below, and generate a new confusion matrix and classification report as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Generalized Linear Model Regression Results                           \n",
      "================================================================================================\n",
      "Dep. Variable:     ['Direction[Down]', 'Direction[Up]']   No. Observations:                  998\n",
      "Model:                                              GLM   Df Residuals:                      995\n",
      "Model Family:                                  Binomial   Df Model:                            2\n",
      "Link Function:                                    logit   Scale:                          1.0000\n",
      "Method:                                            IRLS   Log-Likelihood:                -690.70\n",
      "Date:                                  Sat, 12 Feb 2022   Deviance:                       1381.4\n",
      "Time:                                          20:32:43   Pearson chi2:                     998.\n",
      "No. Iterations:                                       4                                         \n",
      "Covariance Type:                              nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.0322      0.063     -0.508      0.611      -0.156       0.092\n",
      "Lag1           0.0556      0.052      1.076      0.282      -0.046       0.157\n",
      "Lag2           0.0445      0.052      0.861      0.389      -0.057       0.146\n",
      "==============================================================================\n",
      "confusion matrix:\n",
      " [[ 35  35]\n",
      " [ 76 106]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.50      0.32      0.39       111\n",
      "          Up       0.58      0.75      0.66       141\n",
      "\n",
      "    accuracy                           0.56       252\n",
      "   macro avg       0.54      0.53      0.52       252\n",
      "weighted avg       0.55      0.56      0.54       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a model using just lag1 and lag2 and test it (skip the code for the lab)\n",
    "\n",
    "slr = smf.glm(formula='Direction ~ Lag1 + Lag2', data=x_train, family=sm.families.Binomial())\n",
    "slr_fit = slr.fit()\n",
    "print(slr_fit.summary())\n",
    "prob_slr = slr_fit.predict(x_test)\n",
    "pred_slr = ['Up' if x < 0.5 else 'Down' for x in prob_slr]\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_slr).T)\n",
    "print(classification_report(y_test, pred_slr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Questions 1 - 3\n",
    "\n",
    "Question 1: How does the overall accuracy of this smaller model compare\n",
    "\n",
    "Question 2: Show how to use the confusion matrix to derive the overall accuracy as shown in the classification report.\n",
    "(The calculations can be typed here and do not have to be shown with code.)\n",
    "\n",
    "Question 3: How does the interpretability of the second model compare with the first in your opinion? Justify your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1\n",
    "For the smaller model, the accuracy is 0.56. For the bigger model before, the accuracy is only 0.48. The smaller model is more accurate than the bigger one, although the accuracy rate is still not high. \n",
    "\n",
    "Q2\n",
    "smaller model: (35+106)/(35+35+76+106)=141/252=0.5595 \n",
    "bigger model: (77+44)/(77+97+34+44)=121/252=0.4802\n",
    "\n",
    "Q3\n",
    "Because both models are logistic regressions, their interpretability wouldn't differ a lot. The interpretation of the coefficient is that if X increases by 1, the log odd P(down)/P(up) will increase by the value of the coefficient if we hold all other predictors constant. The second model might be more interpretable because there are fewer variables. The bigger model includes the variable volume, which is different from the lag variables. This could make it slightly trickier to interpret. Also, when there are fewer lag variables in the smaller model, the autocorrelation between lag variables would be a less serious issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "We now build a model for the same data with K-Nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN confusion matrix:\n",
      " [[43 58]\n",
      " [68 83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.43      0.39      0.41       111\n",
      "          Up       0.55      0.59      0.57       141\n",
      "\n",
      "    accuracy                           0.50       252\n",
      "   macro avg       0.49      0.49      0.49       252\n",
      "weighted avg       0.50      0.50      0.50       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Restrict the training and test data to only have the 'Lag1' and 'Lag2' predictor variables.\n",
    "# (This code fits the model and makes predictions in one line.)\n",
    "pred = knn.fit(x_train[['Lag1', 'Lag2']], y_train).predict(x_test[['Lag1', 'Lag2']])\n",
    "\n",
    "print('KNN confusion matrix:\\n', confusion_matrix(y_test, pred).T)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN confusion matrix:\n",
      " [[48 55]\n",
      " [63 86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.47      0.43      0.45       111\n",
      "          Up       0.58      0.61      0.59       141\n",
      "\n",
      "    accuracy                           0.53       252\n",
      "   macro avg       0.52      0.52      0.52       252\n",
      "weighted avg       0.53      0.53      0.53       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN with K of 1 performed poorly, let's try K of 3\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "pred = knn.fit(x_train[['Lag1', 'Lag2']], y_train).predict(x_test[['Lag1', 'Lag2']])\n",
    "\n",
    "print('KNN confusion matrix:\\n', confusion_matrix(y_test, pred).T)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Your task: try some more values for K (number of neighbors) and report on which has best overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "KNN confusion matrix:\n",
      " [[43 58]\n",
      " [68 83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.43      0.39      0.41       111\n",
      "          Up       0.55      0.59      0.57       141\n",
      "\n",
      "    accuracy                           0.50       252\n",
      "   macro avg       0.49      0.49      0.49       252\n",
      "weighted avg       0.50      0.50      0.50       252\n",
      "\n",
      "\n",
      "k = 2\n",
      "KNN confusion matrix:\n",
      " [[74 93]\n",
      " [37 48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.44      0.67      0.53       111\n",
      "          Up       0.56      0.34      0.42       141\n",
      "\n",
      "    accuracy                           0.48       252\n",
      "   macro avg       0.50      0.50      0.48       252\n",
      "weighted avg       0.51      0.48      0.47       252\n",
      "\n",
      "\n",
      "k = 3\n",
      "KNN confusion matrix:\n",
      " [[48 55]\n",
      " [63 86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.47      0.43      0.45       111\n",
      "          Up       0.58      0.61      0.59       141\n",
      "\n",
      "    accuracy                           0.53       252\n",
      "   macro avg       0.52      0.52      0.52       252\n",
      "weighted avg       0.53      0.53      0.53       252\n",
      "\n",
      "\n",
      "k = 4\n",
      "KNN confusion matrix:\n",
      " [[71 82]\n",
      " [40 59]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.46      0.64      0.54       111\n",
      "          Up       0.60      0.42      0.49       141\n",
      "\n",
      "    accuracy                           0.52       252\n",
      "   macro avg       0.53      0.53      0.51       252\n",
      "weighted avg       0.54      0.52      0.51       252\n",
      "\n",
      "\n",
      "k = 5\n",
      "KNN confusion matrix:\n",
      " [[40 59]\n",
      " [71 82]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.40      0.36      0.38       111\n",
      "          Up       0.54      0.58      0.56       141\n",
      "\n",
      "    accuracy                           0.48       252\n",
      "   macro avg       0.47      0.47      0.47       252\n",
      "weighted avg       0.48      0.48      0.48       252\n",
      "\n",
      "\n",
      "k = 6\n",
      "KNN confusion matrix:\n",
      " [[63 79]\n",
      " [48 62]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.44      0.57      0.50       111\n",
      "          Up       0.56      0.44      0.49       141\n",
      "\n",
      "    accuracy                           0.50       252\n",
      "   macro avg       0.50      0.50      0.50       252\n",
      "weighted avg       0.51      0.50      0.50       252\n",
      "\n",
      "\n",
      "k = 7\n",
      "KNN confusion matrix:\n",
      " [[41 65]\n",
      " [70 76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.39      0.37      0.38       111\n",
      "          Up       0.52      0.54      0.53       141\n",
      "\n",
      "    accuracy                           0.46       252\n",
      "   macro avg       0.45      0.45      0.45       252\n",
      "weighted avg       0.46      0.46      0.46       252\n",
      "\n",
      "\n",
      "k = 8\n",
      "KNN confusion matrix:\n",
      " [[63 82]\n",
      " [48 59]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.43      0.57      0.49       111\n",
      "          Up       0.55      0.42      0.48       141\n",
      "\n",
      "    accuracy                           0.48       252\n",
      "   macro avg       0.49      0.49      0.48       252\n",
      "weighted avg       0.50      0.48      0.48       252\n",
      "\n",
      "\n",
      "k = 9\n",
      "KNN confusion matrix:\n",
      " [[45 61]\n",
      " [66 80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.42      0.41      0.41       111\n",
      "          Up       0.55      0.57      0.56       141\n",
      "\n",
      "    accuracy                           0.50       252\n",
      "   macro avg       0.49      0.49      0.49       252\n",
      "weighted avg       0.49      0.50      0.49       252\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# That was an improvement, try some other values to compare\n",
    "\n",
    "def print_knn(k):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    pred = knn.fit(x_train[['Lag1', 'Lag2']], y_train).predict(x_test[['Lag1', 'Lag2']])\n",
    "    print('KNN confusion matrix:\\n', confusion_matrix(y_test, pred).T)\n",
    "    print(classification_report(y_test, pred))\n",
    "    \n",
    "for k in range(1,10):\n",
    "    print(\"k =\",k)\n",
    "    print_knn(k)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 4:\n",
    "\n",
    "Question 4: Which of the other K values that you tried for K-Nearest neighbors worked the best, based on overall accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4\n",
    "When k=3, the overall accuracy is the highest among all k from 1 to 10. It is 0.53. \n",
    "k=4 has the second highest accuracy. It is 0.52, only slightly less accurate than k=3. We can see that accuracy increases first and then decreases as k increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Linear discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors: [0.49198397 0.50801603]\n",
      "Means: [[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n",
      "Coefficients: [[-0.05544078 -0.0443452 ]]\n",
      "[[ 35  76]\n",
      " [ 35 106]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.50      0.32      0.39       111\n",
      "          Up       0.58      0.75      0.66       141\n",
      "\n",
      "    accuracy                           0.56       252\n",
      "   macro avg       0.54      0.53      0.52       252\n",
      "weighted avg       0.55      0.56      0.54       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "ldm = lda.fit(x_train[['Lag1', 'Lag2']], y_train)\n",
    "\n",
    "print('Priors:', ldm.priors_)\n",
    "print('Means:', ldm.means_)\n",
    "print('Coefficients:', ldm.coef_)\n",
    "\n",
    "pred = ldm.predict(x_test[['Lag1', 'Lag2']])\n",
    "print(confusion_matrix(pred, y_test).T)\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Quadratic discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors: [0.49198397 0.50801603]\n",
      "Means: [[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n",
      "[[ 30  81]\n",
      " [ 20 121]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.60      0.27      0.37       111\n",
      "          Up       0.60      0.86      0.71       141\n",
      "\n",
      "    accuracy                           0.60       252\n",
      "   macro avg       0.60      0.56      0.54       252\n",
      "weighted avg       0.60      0.60      0.56       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qdm = qda.fit(x_train[['Lag1', 'Lag2']], y_train)\n",
    "\n",
    "print('Priors:', qdm.priors_)\n",
    "print('Means:', qdm.means_)\n",
    "\n",
    "q_pred = qdm.predict(x_test[['Lag1', 'Lag2']])\n",
    "print(confusion_matrix(q_pred, y_test).T)\n",
    "print(classification_report(y_test, q_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 5\n",
    "\n",
    "Question 5: which of the methods that you tried produced the best results for predicting Direction from Lag1 and Lag2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5\n",
    "QDA performs the best. Its accuracy rate is 0.6, higher than LDA (0.56), KNN (0.53 when k=3), and logistic regression (0.56). This suggests that the decision boundary may not be linear and is likely to be quadratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Carseats data\n",
    "\n",
    "Now load the carseats data and try to predict whether the store is located in the US from the other predictor variables.\n",
    "\n",
    "Report below on your findings about (at least) three different learning approaches, comparing their overall accuracy.\n",
    "\n",
    "If you use K-nearest neighbors, be sure to try a few different values for K and report on the best one, showing your work.\n",
    "\n",
    "If you use logistic regression, try to find a simple model with good accuracy by dropping predictors with high p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>CompPrice</th>\n",
       "      <th>Income</th>\n",
       "      <th>Advertising</th>\n",
       "      <th>Population</th>\n",
       "      <th>Price</th>\n",
       "      <th>ShelveLoc</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Urban</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.50</td>\n",
       "      <td>138</td>\n",
       "      <td>73</td>\n",
       "      <td>11</td>\n",
       "      <td>276</td>\n",
       "      <td>120</td>\n",
       "      <td>Bad</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.22</td>\n",
       "      <td>111</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>260</td>\n",
       "      <td>83</td>\n",
       "      <td>Good</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.06</td>\n",
       "      <td>113</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>269</td>\n",
       "      <td>80</td>\n",
       "      <td>Medium</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.40</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>466</td>\n",
       "      <td>97</td>\n",
       "      <td>Medium</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.15</td>\n",
       "      <td>141</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>340</td>\n",
       "      <td>128</td>\n",
       "      <td>Bad</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sales  CompPrice  Income  Advertising  Population  Price ShelveLoc  Age  \\\n",
       "0   9.50        138      73           11         276    120       Bad   42   \n",
       "1  11.22        111      48           16         260     83      Good   65   \n",
       "2  10.06        113      35           10         269     80    Medium   59   \n",
       "3   7.40        117     100            4         466     97    Medium   55   \n",
       "4   4.15        141      64            3         340    128       Bad   38   \n",
       "\n",
       "   Education Urban   US  \n",
       "0         17   Yes  Yes  \n",
       "1         10   Yes  Yes  \n",
       "2         12   Yes  Yes  \n",
       "3         14   Yes  Yes  \n",
       "4         13   Yes   No  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seats = pd.read_csv(DATA_ROOT + 'Carseats.csv')\n",
    "seats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Pick random training and test sets for your analysis:\n",
    "x_train, x_test, y_train, y_test = train_test_split(seats, seats['US'],\n",
    "                                                    train_size=0.8, test_size=0.2)\n",
    "\n",
    "# Hint: if you need to remove some predictors for training or testing in any of the learning methods,\n",
    "# you can use the pandas 'drop' function to drop the corresponding columns, e.g.\n",
    "# x_train.drop(columns=['US']).head()\n",
    "\n",
    "# Hint 2: if you want to write a formula and include a lot of columns, you could use the method\n",
    "# that was shown in lab 1, e.g.:\n",
    "#sm.OLS.from_formula('medv ~ ' + '+'.join(df.columns.difference(['medv', 'age', 'indus'])), df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     ['US[No]', 'US[Yes]']   No. Observations:                  320\n",
      "Model:                               GLM   Df Residuals:                      308\n",
      "Model Family:                   Binomial   Df Model:                           11\n",
      "Link Function:                     logit   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -79.087\n",
      "Date:                   Tue, 15 Feb 2022   Deviance:                       158.17\n",
      "Time:                           19:56:14   Pearson chi2:                     294.\n",
      "No. Iterations:                        8                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept              -0.1429      2.508     -0.057      0.955      -5.058       4.773\n",
      "ShelveLoc[T.Good]      -0.8843      1.130     -0.783      0.434      -3.099       1.330\n",
      "ShelveLoc[T.Medium]    -0.2224      0.637     -0.349      0.727      -1.471       1.026\n",
      "Urban[T.Yes]           -0.1991      0.478     -0.416      0.677      -1.137       0.738\n",
      "Advertising            -0.7896      0.110     -7.198      0.000      -1.005      -0.575\n",
      "Age                     0.0129      0.015      0.843      0.399      -0.017       0.043\n",
      "CompPrice              -0.0065      0.026     -0.250      0.803      -0.057       0.044\n",
      "Education               0.0332      0.078      0.424      0.671      -0.120       0.187\n",
      "Income                 -0.0112      0.008     -1.401      0.161      -0.027       0.004\n",
      "Population              0.0056      0.002      3.361      0.001       0.002       0.009\n",
      "Price                   0.0071      0.022      0.324      0.746      -0.036       0.050\n",
      "Sales                   0.1235      0.194      0.637      0.524      -0.257       0.504\n",
      "=======================================================================================\n",
      "confusion matrix:\n",
      " [[26 10]\n",
      " [ 1 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.96      0.83        27\n",
      "         Yes       0.98      0.81      0.89        53\n",
      "\n",
      "    accuracy                           0.86        80\n",
      "   macro avg       0.85      0.89      0.86        80\n",
      "weighted avg       0.89      0.86      0.87        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here. I would recommend using a different cell for each learning method:\n",
    "\n",
    "# learning method 1: logistic regression\n",
    "# Use all variables in the first logistic regression\n",
    "m1 = smf.glm('US ~ '+ '+'.join(x_train.columns.difference(['US'])), data=x_train, family=sm.families.Binomial())\n",
    "m1_fit = m1.fit()\n",
    "print(m1_fit.summary())\n",
    "\n",
    "prob_m1 = m1_fit.predict(x_test)\n",
    "pred_m1 = ['Yes' if x < 0.5 else 'No' for x in prob_m1]\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_m1).T)\n",
    "print(classification_report(y_test, pred_m1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     ['US[No]', 'US[Yes]']   No. Observations:                  320\n",
      "Model:                               GLM   Df Residuals:                      309\n",
      "Model Family:                   Binomial   Df Model:                           10\n",
      "Link Function:                     logit   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -79.119\n",
      "Date:                   Tue, 15 Feb 2022   Deviance:                       158.24\n",
      "Time:                           20:07:58   Pearson chi2:                     305.\n",
      "No. Iterations:                        8                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept              -0.1744      2.507     -0.070      0.945      -5.089       4.740\n",
      "ShelveLoc[T.Good]      -0.7217      0.924     -0.781      0.435      -2.533       1.090\n",
      "ShelveLoc[T.Medium]    -0.1505      0.568     -0.265      0.791      -1.264       0.963\n",
      "Urban[T.Yes]           -0.2121      0.475     -0.446      0.655      -1.144       0.719\n",
      "Advertising            -0.7859      0.109     -7.241      0.000      -0.999      -0.573\n",
      "Age                     0.0114      0.014      0.810      0.418      -0.016       0.039\n",
      "Education               0.0313      0.078      0.402      0.688      -0.121       0.184\n",
      "Income                 -0.0107      0.008     -1.384      0.166      -0.026       0.004\n",
      "Population              0.0056      0.002      3.395      0.001       0.002       0.009\n",
      "Price                   0.0024      0.011      0.214      0.830      -0.020       0.025\n",
      "Sales                   0.0885      0.134      0.659      0.510      -0.175       0.352\n",
      "=======================================================================================\n",
      "confusion matrix:\n",
      " [[26 10]\n",
      " [ 1 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.96      0.83        27\n",
      "         Yes       0.98      0.81      0.89        53\n",
      "\n",
      "    accuracy                           0.86        80\n",
      "   macro avg       0.85      0.89      0.86        80\n",
      "weighted avg       0.89      0.86      0.87        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning method 1: logistic regression\n",
    "# drop the variable with the highest p-value: CompPrice\n",
    "m2 = smf.glm('US ~ '+ '+'.join(x_train.columns.difference(['US','CompPrice'])), data=x_train, family=sm.families.Binomial())\n",
    "m2_fit = m2.fit()\n",
    "print(m2_fit.summary())\n",
    "\n",
    "prob_m2 = m2_fit.predict(x_test)\n",
    "pred_m2 = ['Yes' if x < 0.5 else 'No' for x in prob_m2]\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_m2).T)\n",
    "print(classification_report(y_test, pred_m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     ['US[No]', 'US[Yes]']   No. Observations:                  320\n",
      "Model:                               GLM   Df Residuals:                      310\n",
      "Model Family:                   Binomial   Df Model:                            9\n",
      "Link Function:                     logit   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -79.141\n",
      "Date:                   Tue, 15 Feb 2022   Deviance:                       158.28\n",
      "Time:                           20:09:10   Pearson chi2:                     306.\n",
      "No. Iterations:                        8                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept               0.2305      1.650      0.140      0.889      -3.003       3.464\n",
      "ShelveLoc[T.Good]      -0.6141      0.774     -0.793      0.428      -2.132       0.903\n",
      "ShelveLoc[T.Medium]    -0.1111      0.537     -0.207      0.836      -1.163       0.941\n",
      "Urban[T.Yes]           -0.2203      0.474     -0.465      0.642      -1.149       0.708\n",
      "Advertising            -0.7809      0.105     -7.404      0.000      -0.988      -0.574\n",
      "Age                     0.0104      0.013      0.785      0.432      -0.016       0.036\n",
      "Education               0.0328      0.078      0.422      0.673      -0.120       0.185\n",
      "Income                 -0.0105      0.008     -1.369      0.171      -0.026       0.005\n",
      "Population              0.0056      0.002      3.397      0.001       0.002       0.009\n",
      "Sales                   0.0687      0.097      0.706      0.480      -0.122       0.260\n",
      "=======================================================================================\n",
      "confusion matrix:\n",
      " [[26 10]\n",
      " [ 1 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.96      0.83        27\n",
      "         Yes       0.98      0.81      0.89        53\n",
      "\n",
      "    accuracy                           0.86        80\n",
      "   macro avg       0.85      0.89      0.86        80\n",
      "weighted avg       0.89      0.86      0.87        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning method 1: logistic regression\n",
    "# drop the variable with the next highest p-value: Price\n",
    "m3 = smf.glm('US ~ '+ '+'.join(x_train.columns.difference(['US','CompPrice','Price'])), data=x_train, family=sm.families.Binomial())\n",
    "m3_fit = m3.fit()\n",
    "print(m3_fit.summary())\n",
    "\n",
    "prob_m3 = m3_fit.predict(x_test)\n",
    "pred_m3 = ['Yes' if x < 0.5 else 'No' for x in prob_m3]\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_m3).T)\n",
    "print(classification_report(y_test, pred_m3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     ['US[No]', 'US[Yes]']   No. Observations:                  320\n",
      "Model:                               GLM   Df Residuals:                      312\n",
      "Model Family:                   Binomial   Df Model:                            7\n",
      "Link Function:                     logit   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -79.534\n",
      "Date:                   Tue, 15 Feb 2022   Deviance:                       159.07\n",
      "Time:                           20:09:48   Pearson chi2:                     326.\n",
      "No. Iterations:                        8                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept        0.3027      1.643      0.184      0.854      -2.918       3.524\n",
      "Urban[T.Yes]    -0.2071      0.470     -0.440      0.660      -1.129       0.714\n",
      "Advertising     -0.7747      0.104     -7.421      0.000      -0.979      -0.570\n",
      "Age              0.0097      0.013      0.755      0.450      -0.016       0.035\n",
      "Education        0.0425      0.077      0.554      0.580      -0.108       0.193\n",
      "Income          -0.0110      0.008     -1.431      0.152      -0.026       0.004\n",
      "Population       0.0055      0.002      3.385      0.001       0.002       0.009\n",
      "Sales            0.0221      0.075      0.292      0.770      -0.126       0.170\n",
      "================================================================================\n",
      "confusion matrix:\n",
      " [[26 10]\n",
      " [ 1 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.96      0.83        27\n",
      "         Yes       0.98      0.81      0.89        53\n",
      "\n",
      "    accuracy                           0.86        80\n",
      "   macro avg       0.85      0.89      0.86        80\n",
      "weighted avg       0.89      0.86      0.87        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning method 1: logistic regression\n",
    "# drop the variable with the next highest p-value: ShelveLoc\n",
    "m4 = smf.glm('US ~ '+ '+'.join(x_train.columns.difference(['US','CompPrice','Price','ShelveLoc'])), data=x_train, family=sm.families.Binomial())\n",
    "m4_fit = m4.fit()\n",
    "print(m4_fit.summary())\n",
    "\n",
    "prob_m4 = m4_fit.predict(x_test)\n",
    "pred_m4 = ['Yes' if x < 0.5 else 'No' for x in prob_m4]\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_m4).T)\n",
    "print(classification_report(y_test, pred_m4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     ['US[No]', 'US[Yes]']   No. Observations:                  320\n",
      "Model:                               GLM   Df Residuals:                      313\n",
      "Model Family:                   Binomial   Df Model:                            6\n",
      "Link Function:                     logit   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -79.577\n",
      "Date:                   Tue, 15 Feb 2022   Deviance:                       159.15\n",
      "Time:                           20:10:51   Pearson chi2:                     332.\n",
      "No. Iterations:                        8                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept        0.4866      1.517      0.321      0.748      -2.486       3.459\n",
      "Urban[T.Yes]    -0.1933      0.468     -0.414      0.679      -1.110       0.723\n",
      "Advertising     -0.7732      0.104     -7.413      0.000      -0.978      -0.569\n",
      "Age              0.0087      0.012      0.701      0.483      -0.016       0.033\n",
      "Education        0.0429      0.077      0.560      0.576      -0.107       0.193\n",
      "Income          -0.0109      0.008     -1.427      0.154      -0.026       0.004\n",
      "Population       0.0055      0.002      3.392      0.001       0.002       0.009\n",
      "================================================================================\n",
      "confusion matrix:\n",
      " [[26 10]\n",
      " [ 1 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.96      0.83        27\n",
      "         Yes       0.98      0.81      0.89        53\n",
      "\n",
      "    accuracy                           0.86        80\n",
      "   macro avg       0.85      0.89      0.86        80\n",
      "weighted avg       0.89      0.86      0.87        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning method 1: logistic regression\n",
    "# drop the variable with the next highest p-value: Sales\n",
    "m5 = smf.glm('US ~ '+ '+'.join(x_train.columns.difference(['US','CompPrice','Price','ShelveLoc','Sales'])), data=x_train, family=sm.families.Binomial())\n",
    "m5_fit = m5.fit()\n",
    "print(m5_fit.summary())\n",
    "\n",
    "prob_m5 = m5_fit.predict(x_test)\n",
    "pred_m5 = ['Yes' if x < 0.5 else 'No' for x in prob_m5]\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_m5).T)\n",
    "print(classification_report(y_test, pred_m5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     ['US[No]', 'US[Yes]']   No. Observations:                  320\n",
      "Model:                               GLM   Df Residuals:                      314\n",
      "Model Family:                   Binomial   Df Model:                            5\n",
      "Link Function:                     logit   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -79.663\n",
      "Date:                   Tue, 15 Feb 2022   Deviance:                       159.33\n",
      "Time:                           20:11:32   Pearson chi2:                     354.\n",
      "No. Iterations:                        8                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "Intercept       0.3140      1.458      0.215      0.829      -2.543       3.171\n",
      "Advertising    -0.7778      0.105     -7.441      0.000      -0.983      -0.573\n",
      "Age             0.0087      0.012      0.702      0.483      -0.016       0.033\n",
      "Education       0.0451      0.076      0.590      0.555      -0.105       0.195\n",
      "Income         -0.0110      0.008     -1.433      0.152      -0.026       0.004\n",
      "Population      0.0056      0.002      3.418      0.001       0.002       0.009\n",
      "===============================================================================\n",
      "confusion matrix:\n",
      " [[26 10]\n",
      " [ 1 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.96      0.83        27\n",
      "         Yes       0.98      0.81      0.89        53\n",
      "\n",
      "    accuracy                           0.86        80\n",
      "   macro avg       0.85      0.89      0.86        80\n",
      "weighted avg       0.89      0.86      0.87        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning method 1: logistic regression\n",
    "# drop the variable with the next highest p-value: Urban\n",
    "m6 = smf.glm('US ~ '+ '+'.join(x_train.columns.difference(['US','CompPrice','Price','ShelveLoc','Urban','Sales'])), data=x_train, family=sm.families.Binomial())\n",
    "m6_fit = m6.fit()\n",
    "print(m6_fit.summary())\n",
    "\n",
    "prob_m6 = m6_fit.predict(x_test)\n",
    "pred_m6 = ['Yes' if x < 0.5 else 'No' for x in prob_m6]\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_m6).T)\n",
    "print(classification_report(y_test, pred_m6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     ['US[No]', 'US[Yes]']   No. Observations:                  320\n",
      "Model:                               GLM   Df Residuals:                      315\n",
      "Model Family:                   Binomial   Df Model:                            4\n",
      "Link Function:                     logit   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -79.837\n",
      "Date:                   Tue, 15 Feb 2022   Deviance:                       159.67\n",
      "Time:                           20:14:29   Pearson chi2:                     383.\n",
      "No. Iterations:                        8                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "Intercept       0.9998      0.891      1.122      0.262      -0.747       2.746\n",
      "Advertising    -0.7800      0.104     -7.480      0.000      -0.984      -0.576\n",
      "Age             0.0083      0.012      0.672      0.501      -0.016       0.033\n",
      "Income         -0.0112      0.008     -1.472      0.141      -0.026       0.004\n",
      "Population      0.0055      0.002      3.403      0.001       0.002       0.009\n",
      "===============================================================================\n",
      "confusion matrix:\n",
      " [[26 10]\n",
      " [ 1 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.96      0.83        27\n",
      "         Yes       0.98      0.81      0.89        53\n",
      "\n",
      "    accuracy                           0.86        80\n",
      "   macro avg       0.85      0.89      0.86        80\n",
      "weighted avg       0.89      0.86      0.87        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning method 1: logistic regression\n",
    "# drop the variable with the next highest p-value: Education\n",
    "m7 = smf.glm('US ~ '+ '+'.join(x_train.columns.difference(['US','CompPrice','Price','ShelveLoc','Urban','Sales','Education'])), data=x_train, family=sm.families.Binomial())\n",
    "m7_fit = m7.fit()\n",
    "print(m7_fit.summary())\n",
    "\n",
    "prob_m7 = m7_fit.predict(x_test)\n",
    "pred_m7 = ['Yes' if x < 0.5 else 'No' for x in prob_m7]\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_m7).T)\n",
    "print(classification_report(y_test, pred_m7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     ['US[No]', 'US[Yes]']   No. Observations:                  320\n",
      "Model:                               GLM   Df Residuals:                      316\n",
      "Model Family:                   Binomial   Df Model:                            3\n",
      "Link Function:                     logit   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -80.064\n",
      "Date:                   Tue, 15 Feb 2022   Deviance:                       160.13\n",
      "Time:                           20:15:04   Pearson chi2:                     435.\n",
      "No. Iterations:                        8                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "Intercept       1.4128      0.650      2.173      0.030       0.139       2.687\n",
      "Advertising    -0.7789      0.104     -7.489      0.000      -0.983      -0.575\n",
      "Income         -0.0107      0.008     -1.418      0.156      -0.025       0.004\n",
      "Population      0.0054      0.002      3.370      0.001       0.002       0.009\n",
      "===============================================================================\n",
      "confusion matrix:\n",
      " [[26 10]\n",
      " [ 1 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.96      0.83        27\n",
      "         Yes       0.98      0.81      0.89        53\n",
      "\n",
      "    accuracy                           0.86        80\n",
      "   macro avg       0.85      0.89      0.86        80\n",
      "weighted avg       0.89      0.86      0.87        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning method 1: logistic regression\n",
    "# drop the variable with the next highest p-value: Age\n",
    "m8 = smf.glm('US ~ '+ '+'.join(x_train.columns.difference(['US','CompPrice','Price','ShelveLoc','Urban','Sales','Education','Age'])), data=x_train, family=sm.families.Binomial())\n",
    "m8_fit = m8.fit()\n",
    "print(m8_fit.summary())\n",
    "\n",
    "prob_m8 = m8_fit.predict(x_test)\n",
    "pred_m8 = ['Yes' if x < 0.5 else 'No' for x in prob_m8]\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_m8).T)\n",
    "print(classification_report(y_test, pred_m8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     ['US[No]', 'US[Yes]']   No. Observations:                  320\n",
      "Model:                               GLM   Df Residuals:                      317\n",
      "Model Family:                   Binomial   Df Model:                            2\n",
      "Link Function:                     logit   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -81.089\n",
      "Date:                   Tue, 15 Feb 2022   Deviance:                       162.18\n",
      "Time:                           20:16:00   Pearson chi2:                     473.\n",
      "No. Iterations:                        8                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "Intercept       0.6414      0.341      1.883      0.060      -0.026       1.309\n",
      "Advertising    -0.7688      0.101     -7.580      0.000      -0.968      -0.570\n",
      "Population      0.0054      0.002      3.386      0.001       0.002       0.008\n",
      "===============================================================================\n",
      "confusion matrix:\n",
      " [[26 10]\n",
      " [ 1 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.96      0.83        27\n",
      "         Yes       0.98      0.81      0.89        53\n",
      "\n",
      "    accuracy                           0.86        80\n",
      "   macro avg       0.85      0.89      0.86        80\n",
      "weighted avg       0.89      0.86      0.87        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning method 1: logistic regression\n",
    "# drop the variable with the next highest p-value: Income\n",
    "m9 = smf.glm('US ~ '+ '+'.join(x_train.columns.difference(['US','CompPrice','Price','ShelveLoc','Urban','Sales','Education','Age','Income'])), data=x_train, family=sm.families.Binomial())\n",
    "m9_fit = m9.fit()\n",
    "print(m9_fit.summary())\n",
    "\n",
    "prob_m9 = m8_fit.predict(x_test)\n",
    "pred_m9 = ['Yes' if x < 0.5 else 'No' for x in prob_m9]\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_m9).T)\n",
    "print(classification_report(y_test, pred_m9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     ['US[No]', 'US[Yes]']   No. Observations:                  320\n",
      "Model:                               GLM   Df Residuals:                      317\n",
      "Model Family:                   Binomial   Df Model:                            2\n",
      "Link Function:                     logit   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -81.089\n",
      "Date:                   Tue, 15 Feb 2022   Deviance:                       162.18\n",
      "Time:                           20:18:36   Pearson chi2:                     473.\n",
      "No. Iterations:                        8                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "Intercept       0.6414      0.341      1.883      0.060      -0.026       1.309\n",
      "Advertising    -0.7688      0.101     -7.580      0.000      -0.968      -0.570\n",
      "Population      0.0054      0.002      3.386      0.001       0.002       0.008\n",
      "===============================================================================\n",
      "confusion matrix:\n",
      " [[26 10]\n",
      " [ 1 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.72      0.96      0.83        27\n",
      "         Yes       0.98      0.81      0.89        53\n",
      "\n",
      "    accuracy                           0.86        80\n",
      "   macro avg       0.85      0.89      0.86        80\n",
      "weighted avg       0.89      0.86      0.87        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning method 1: logistic regression\n",
    "# From backward selection, we identify advertising and population as the two key variables\n",
    "# Note that accuracy does not drop as we drop variables\n",
    "# Below is our final model\n",
    "lg = smf.glm('US ~ Advertising+Population', data=x_train, family=sm.families.Binomial())\n",
    "lg_fit = lg.fit()\n",
    "print(lg_fit.summary())\n",
    "\n",
    "prob_lg = lg_fit.predict(x_test)\n",
    "pred_lg = ['Yes' if x < 0.5 else 'No' for x in prob_lg]\n",
    "print('confusion matrix:\\n', confusion_matrix(y_test, pred_lg).T)\n",
    "print(classification_report(y_test, pred_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "KNN confusion matrix:\n",
      " [[22 10]\n",
      " [ 5 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.69      0.81      0.75        27\n",
      "         Yes       0.90      0.81      0.85        53\n",
      "\n",
      "    accuracy                           0.81        80\n",
      "   macro avg       0.79      0.81      0.80        80\n",
      "weighted avg       0.83      0.81      0.82        80\n",
      "\n",
      "\n",
      "k = 2\n",
      "KNN confusion matrix:\n",
      " [[25 14]\n",
      " [ 2 39]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.64      0.93      0.76        27\n",
      "         Yes       0.95      0.74      0.83        53\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.80      0.83      0.79        80\n",
      "weighted avg       0.85      0.80      0.81        80\n",
      "\n",
      "\n",
      "k = 3\n",
      "KNN confusion matrix:\n",
      " [[20  9]\n",
      " [ 7 44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.69      0.74      0.71        27\n",
      "         Yes       0.86      0.83      0.85        53\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.78      0.79      0.78        80\n",
      "weighted avg       0.80      0.80      0.80        80\n",
      "\n",
      "\n",
      "k = 4\n",
      "KNN confusion matrix:\n",
      " [[25 11]\n",
      " [ 2 42]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.69      0.93      0.79        27\n",
      "         Yes       0.95      0.79      0.87        53\n",
      "\n",
      "    accuracy                           0.84        80\n",
      "   macro avg       0.82      0.86      0.83        80\n",
      "weighted avg       0.87      0.84      0.84        80\n",
      "\n",
      "\n",
      "k = 5\n",
      "KNN confusion matrix:\n",
      " [[21 10]\n",
      " [ 6 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.68      0.78      0.72        27\n",
      "         Yes       0.88      0.81      0.84        53\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.78      0.79      0.78        80\n",
      "weighted avg       0.81      0.80      0.80        80\n",
      "\n",
      "\n",
      "k = 6\n",
      "KNN confusion matrix:\n",
      " [[24 13]\n",
      " [ 3 40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.65      0.89      0.75        27\n",
      "         Yes       0.93      0.75      0.83        53\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.79      0.82      0.79        80\n",
      "weighted avg       0.84      0.80      0.81        80\n",
      "\n",
      "\n",
      "k = 7\n",
      "KNN confusion matrix:\n",
      " [[14 11]\n",
      " [13 42]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.56      0.52      0.54        27\n",
      "         Yes       0.76      0.79      0.78        53\n",
      "\n",
      "    accuracy                           0.70        80\n",
      "   macro avg       0.66      0.66      0.66        80\n",
      "weighted avg       0.69      0.70      0.70        80\n",
      "\n",
      "\n",
      "k = 8\n",
      "KNN confusion matrix:\n",
      " [[17 13]\n",
      " [10 40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.57      0.63      0.60        27\n",
      "         Yes       0.80      0.75      0.78        53\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.68      0.69      0.69        80\n",
      "weighted avg       0.72      0.71      0.72        80\n",
      "\n",
      "\n",
      "k = 9\n",
      "KNN confusion matrix:\n",
      " [[13  9]\n",
      " [14 44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.59      0.48      0.53        27\n",
      "         Yes       0.76      0.83      0.79        53\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.67      0.66      0.66        80\n",
      "weighted avg       0.70      0.71      0.70        80\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning method 2: KNN\n",
    "# Include advertising and population as predictors\n",
    "def print_knn(k):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    pred = knn.fit(x_train[['Advertising', 'Population']], y_train).predict(x_test[['Advertising','Population']])\n",
    "    print('KNN confusion matrix:\\n', confusion_matrix(y_test, pred).T)\n",
    "    print(classification_report(y_test, pred))\n",
    "    \n",
    "for k in range(1,10):\n",
    "    print(\"k =\",k)\n",
    "    print_knn(k)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors: [0.359375 0.640625]\n",
      "Means: [[  0.55652174 246.06086957]\n",
      " [ 10.20487805 268.76097561]]\n",
      "Coefficients: [[ 0.43865566 -0.00362916]]\n",
      "[[26  1]\n",
      " [12 41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.68      0.96      0.80        27\n",
      "         Yes       0.98      0.77      0.86        53\n",
      "\n",
      "    accuracy                           0.84        80\n",
      "   macro avg       0.83      0.87      0.83        80\n",
      "weighted avg       0.88      0.84      0.84        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning method 3: LDA\n",
    "# Include advertising, income, and population as predictors\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "ldm = lda.fit(x_train[['Advertising','Population']], y_train)\n",
    "\n",
    "print('Priors:', ldm.priors_)\n",
    "print('Means:', ldm.means_)\n",
    "print('Coefficients:', ldm.coef_)\n",
    "\n",
    "pred = ldm.predict(x_test[['Advertising','Population']])\n",
    "print(confusion_matrix(pred, y_test).T)\n",
    "print(classification_report(y_test, pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Questions 6-9\n",
    "\n",
    "(Each of the three questions below carries the same weight as the earlier questions.)\n",
    "\n",
    "Question 6: What was the first method you tried, and what was its best overall accuracy?\n",
    "\n",
    "Question 7: What was the second method you tried, and what was its best overall accuracy?\n",
    "\n",
    "Question 8: What was the third method you tried, and what was its best overall accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all my models, I only include advertising and population as predictors. These two predictors have the lowest p-values in the logistic regression.\n",
    "\n",
    "Q6\n",
    "The first one is logistic regression. It is pretty accurate. The accuracy is 0.86 in the simpler model with only two predictors. This accuracy is the highest among all three methods I try.\n",
    "\n",
    "Q7\n",
    "The second one is KNN. It is less accurate than logistic regression but still quite accurate. I try k=1 to k=10. The accuracy is the highest when k=4. It is 0.84. As k increases, accuracy increases first and then decreases, meaning that the complicated models would overfit the data, suggesting that the decision boundary in this example is simple (close to linear).\n",
    "\n",
    "Q8\n",
    "The third is LDA. Its accuracy is also very high. It is 0.84. The accuracy is slightly less than logistic regression. This suggests that the decision boundary may be approximately linear since LR and LDA both perform well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
